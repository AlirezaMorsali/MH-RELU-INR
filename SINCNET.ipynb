{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bittensorflow2conda6db7451bca3f4ef3ad8ecc62d6c504c4",
   "display_name": "Python 3.7.9 64-bit ('TensorFlow2': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import MaxPooling1D, Conv1D, LeakyReLU, BatchNormalization, Dense, Flatten\n",
    "from tensorflow.keras.layers import InputLayer, Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TensorFlow Version : 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow Version : {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(Layer):\n",
    "    \"\"\" Layer Normalization in the style of https://arxiv.org/abs/1607.06450 \"\"\"\n",
    "\n",
    "    def __init__(self, scale_initializer='ones', bias_initializer='zeros', **kwargs):\n",
    "        super(LayerNorm, self).__init__(**kwargs)\n",
    "        self.epsilon = 1e-6\n",
    "        self.scale_initializer = initializers.get(scale_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.scale = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer=self.scale_initializer,\n",
    "                                     trainable=True,\n",
    "                                     name='{}_scale'.format(self.name))\n",
    "        self.bias = self.add_weight(shape=(input_shape[-1],),\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    trainable=True,\n",
    "                                    name='{}_bias'.format(self.name))\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        norm = (x - mean) * (1 / (std + self.epsilon))\n",
    "        return norm * self.scale + self.bias\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_print(*objects):\n",
    "    if debug:\n",
    "        print(*objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinc(band, t_right):\n",
    "    y_right = K.sin(2 * math.pi * band * t_right) / (2 * math.pi * band * t_right)\n",
    "    # y_left = flip(y_right, 0) TODO remove if useless\n",
    "    y_left = K.reverse(y_right, 0)\n",
    "    #y = K.concatenate([y_left, K.variable(K.ones(1)), y_right])\n",
    "    y = K.concatenate([y_left, K.ones(1), y_right])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SincConv1D(Layer):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            N_filt,\n",
    "            Filt_dim,\n",
    "            fs,\n",
    "            **kwargs):\n",
    "        self.N_filt = N_filt\n",
    "        self.Filt_dim = Filt_dim\n",
    "        self.fs = fs\n",
    "\n",
    "        super(SincConv1D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # The filters are trainable parameters.\n",
    "        self.filt_b1 = self.add_weight(\n",
    "            name='filt_b1',\n",
    "            shape=(self.N_filt,),\n",
    "            initializer='uniform',\n",
    "            trainable=True)\n",
    "        self.filt_band = self.add_weight(\n",
    "            name='filt_band',\n",
    "            shape=(self.N_filt,),\n",
    "            initializer='uniform',\n",
    "            trainable=True)\n",
    "\n",
    "        # Mel Initialization of the filterbanks\n",
    "        low_freq_mel = 80\n",
    "        high_freq_mel = (2595 * np.log10(1 + (self.fs / 2) / 700))  # Convert Hz to Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.N_filt)  # Equally spaced in Mel scale\n",
    "        f_cos = (700 * (10 ** (mel_points / 2595) - 1))  # Convert Mel to Hz\n",
    "        b1 = np.roll(f_cos, 1)\n",
    "        b2 = np.roll(f_cos, -1)\n",
    "        b1[0] = 30\n",
    "        b2[-1] = (self.fs / 2) - 100\n",
    "        self.B1 = b1\n",
    "        self.B2 = b2\n",
    "        self.freq_scale = self.fs * 1.0\n",
    "        self.set_weights([b1 / self.freq_scale, (b2 - b1) / self.freq_scale])\n",
    "\n",
    "        # Get beginning and end frequencies of the filters.\n",
    "        min_freq = 50.0\n",
    "        min_band = 50.0\n",
    "        self.filt_beg_freq = K.abs(self.filt_b1) + min_freq / self.freq_scale\n",
    "        self.filt_end_freq = self.filt_beg_freq + (K.abs(self.filt_band) + min_band / self.freq_scale)\n",
    "\n",
    "        # Filter window (hamming).\n",
    "        n = np.linspace(0, self.Filt_dim, self.Filt_dim)\n",
    "        window = 0.54 - 0.46 * K.cos(2 * math.pi * n / self.Filt_dim)\n",
    "        window = K.cast(window, \"float32\")\n",
    "        #self.window = K.variable(window)\n",
    "        self.window = K.constant(window)\n",
    "        debug_print(\"  window\", self.window.shape)\n",
    "\n",
    "        # tODO what is this?\n",
    "        t_right_linspace = np.linspace(1, (self.Filt_dim - 1) / 2, int((self.Filt_dim - 1) / 2))\n",
    "        #self.t_right = K.variable(t_right_linspace / self.fs)\n",
    "        self.t_right = K.constant(t_right_linspace / self.fs)\n",
    "        debug_print(\"  t_right\", self.t_right)\n",
    "\n",
    "        super(SincConv1D, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        debug_print(\"call\")\n",
    "        # filters = K.zeros(shape=(N_filt, Filt_dim))\n",
    "\n",
    "        # Compute the filters.\n",
    "        output_list = []\n",
    "        for i in range(self.N_filt):\n",
    "            low_pass1 = 2 * self.filt_beg_freq[i] * sinc(self.filt_beg_freq[i] * self.freq_scale, self.t_right)\n",
    "            low_pass2 = 2 * self.filt_end_freq[i] * sinc(self.filt_end_freq[i] * self.freq_scale, self.t_right)\n",
    "            band_pass = (low_pass2 - low_pass1)\n",
    "            band_pass = band_pass / K.max(band_pass)\n",
    "            output_list.append(band_pass * self.window)\n",
    "        filters = K.stack(output_list)  # (80, 251)\n",
    "        filters = K.transpose(filters)  # (251, 80)\n",
    "        filters = K.reshape(filters, (self.Filt_dim, 1,\n",
    "                                      self.N_filt))  # (251,1,80) in TF: (filter_width, in_channels, out_channels) in\n",
    "        # PyTorch (out_channels, in_channels, filter_width)\n",
    "\n",
    "        '''Given an input tensor of shape [batch, in_width, in_channels] if data_format is \"NWC\", or [batch, \n",
    "        in_channels, in_width] if data_format is \"NCW\", and a filter / kernel tensor of shape [filter_width, \n",
    "        in_channels, out_channels], this op reshapes the arguments to pass them to conv2d to perform the equivalent \n",
    "        convolution operation. Internally, this op reshapes the input tensors and invokes tf.nn.conv2d. For example, \n",
    "        if data_format does not start with \"NC\", a tensor of shape [batch, in_width, in_channels] is reshaped to [\n",
    "        batch, 1, in_width, in_channels], and the filter is reshaped to [1, filter_width, in_channels, out_channels]. \n",
    "        The result is then reshaped back to [batch, out_width, out_channels] (where out_width is a function of the \n",
    "        stride and padding as in conv2d) and returned to the caller. '''\n",
    "\n",
    "        # Do the convolution.\n",
    "        debug_print(\"call\")\n",
    "        debug_print(\"  x\", x)\n",
    "        debug_print(\"  filters\", filters)\n",
    "        out = K.conv1d(\n",
    "            x,\n",
    "            kernel=filters\n",
    "        )\n",
    "        debug_print(\"  out\", out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        new_size = conv_utils.conv_output_length(\n",
    "            input_shape[1],\n",
    "            self.Filt_dim,\n",
    "            padding=\"valid\",\n",
    "            stride=1,\n",
    "            dilation=1)\n",
    "        return (input_shape[0],) + (new_size,) + (self.N_filt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape, out_dim):\n",
    "    #\n",
    "    inputs = Input(input_shape)\n",
    "    x = SincConv1D(80, 251, 16000)(inputs)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=3)(x)\n",
    "    if False:\n",
    "        x = BatchNormalization(momentum=0.05)(x)\n",
    "    if True:\n",
    "        x = LayerNorm()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(60, 5, strides=1, padding='valid')(x)\n",
    "    x = MaxPooling1D(pool_size=3)(x)\n",
    "    if False:\n",
    "        x = BatchNormalization(momentum=0.05)(x)\n",
    "    if True:\n",
    "        x = LayerNorm()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(60, 5, strides=1, padding='valid')(x)\n",
    "    x = MaxPooling1D(pool_size=3)(x)\n",
    "    if False:\n",
    "        x = BatchNormalization(momentum=0.05)(x)\n",
    "    if True:\n",
    "        x = LayerNorm()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # DNN\n",
    "    x = Dense(256)(x)\n",
    "    if True:\n",
    "        x = BatchNormalization(momentum=0.05, epsilon=1e-5)(x)\n",
    "    if False:\n",
    "        x = LayerNorm()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Dense(256)(x)\n",
    "    if True:\n",
    "        x = BatchNormalization(momentum=0.05, epsilon=1e-5)(x)\n",
    "    if False:\n",
    "        x = LayerNorm()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Dense(256)(x)\n",
    "    if True:\n",
    "        x = BatchNormalization(momentum=0.05, epsilon=1e-5)(x)\n",
    "    if False:\n",
    "        x = LayerNorm()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # DNN final\n",
    "    prediction = layers.Dense(out_dim, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=prediction)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 3200, 1)]         0         \n_________________________________________________________________\nsinc_conv1d_12 (SincConv1D)  (None, 2950, 80)          160       \n_________________________________________________________________\nmax_pooling1d_6 (MaxPooling1 (None, 983, 80)           0         \n_________________________________________________________________\nlayer_norm_6 (LayerNorm)     (None, 983, 80)           160       \n_________________________________________________________________\nleaky_re_lu_12 (LeakyReLU)   (None, 983, 80)           0         \n_________________________________________________________________\nconv1d_4 (Conv1D)            (None, 979, 60)           24060     \n_________________________________________________________________\nmax_pooling1d_7 (MaxPooling1 (None, 326, 60)           0         \n_________________________________________________________________\nlayer_norm_7 (LayerNorm)     (None, 326, 60)           120       \n_________________________________________________________________\nleaky_re_lu_13 (LeakyReLU)   (None, 326, 60)           0         \n_________________________________________________________________\nconv1d_5 (Conv1D)            (None, 322, 60)           18060     \n_________________________________________________________________\nmax_pooling1d_8 (MaxPooling1 (None, 107, 60)           0         \n_________________________________________________________________\nlayer_norm_8 (LayerNorm)     (None, 107, 60)           120       \n_________________________________________________________________\nleaky_re_lu_14 (LeakyReLU)   (None, 107, 60)           0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 6420)              0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 256)               1643776   \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 256)               1024      \n_________________________________________________________________\nleaky_re_lu_15 (LeakyReLU)   (None, 256)               0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 256)               65792     \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 256)               1024      \n_________________________________________________________________\nleaky_re_lu_16 (LeakyReLU)   (None, 256)               0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 256)               65792     \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 256)               1024      \n_________________________________________________________________\nleaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 12)                3084      \n=================================================================\nTotal params: 1,824,196\nTrainable params: 1,822,660\nNon-trainable params: 1,536\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model((3200,1), 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}